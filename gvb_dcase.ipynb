{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5065d4a-36c7-49d5-a2be-06a55b5af2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio: [-0.005696    0.00483759  0.01827622 ...  0.02230001  0.01699919\n",
      "  0.01644135]\n",
      "Shape of audio data: (220500,)\n",
      "Sample rate: 22050\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch.nn as nn\n",
    "\n",
    "audio_path = 'G3/data/corrected_split/test/source/airport-barcelona-203-6132-a.wav'\n",
    "y, sr = librosa.load(audio_path)\n",
    "audio=y\n",
    "print(f\"audio: {y}\")\n",
    "print(f\"Shape of audio data: {y.shape}\")\n",
    "print(f\"Sample rate: {sr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ef572d-3e86-4cf7-ad01-4a5301f602e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hear21passt.base import get_basic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962e0137-68ab-4356-9164-cefefde70bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PaSSTFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        super(PaSSTFeatureExtractor, self).__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = get_basic_model(mode=\"embed_only\") \n",
    "        self.model.to(self.device)\n",
    "       \n",
    "\n",
    "    def forward(self, audio_waveform, sample_rate=32000):\n",
    "     \n",
    "        if audio_waveform.dim() == 1:\n",
    "            audio_waveform = audio_waveform.unsqueeze(0)  # -e \n",
    "        # print(\"PaSST extractor got shape:\", audio_waveform.shape)    \n",
    "        audio_waveform = audio_waveform.to(self.device)        \n",
    "        features = self.model(audio_waveform)\n",
    "             \n",
    "        return features\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca5fe39-3504-464e-9103-6a8ac119bc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n",
      "x torch.Size([1, 1, 128, 690])\n",
      "self.norm(x) torch.Size([1, 768, 12, 68])\n",
      " patch_embed :  torch.Size([1, 768, 12, 68])\n",
      " self.time_new_pos_embed.shape torch.Size([1, 768, 1, 99])\n",
      " CUT time_new_pos_embed.shape torch.Size([1, 768, 1, 68])\n",
      " self.freq_new_pos_embed.shape torch.Size([1, 768, 12, 1])\n",
      "X flattened torch.Size([1, 816, 768])\n",
      " self.new_pos_embed.shape torch.Size([1, 2, 768])\n",
      " self.cls_tokens.shape torch.Size([1, 1, 768])\n",
      " self.dist_token.shape torch.Size([1, 1, 768])\n",
      " final sequence x torch.Size([1, 818, 768])\n",
      " after 12 atten blocks x torch.Size([1, 818, 768])\n",
      "forward_features torch.Size([1, 768])\n",
      "head torch.Size([1, 527])\n",
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teaching/anaconda3/lib/python3.12/site-packages/torch/functional.py:730: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /pytorch/aten/src/ATen/native/SpectralOps.cpp:875.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n",
      "/home/teaching/anaconda3/lib/python3.12/site-packages/hear21passt/models/preprocess.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/teaching/anaconda3/lib/python3.12/site-packages/hear21passt/models/passt.py:304: UserWarning: Input image size (128*690) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n"
     ]
    }
   ],
   "source": [
    "extractor=PaSSTFeatureExtractor()\n",
    "audio_tensor=torch.tensor(y,dtype=torch.float32)\n",
    "features=extractor(audio_tensor)\n",
    "# print(features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377547c0-dc46-432b-a125-3b9dce312b72",
   "metadata": {},
   "source": [
    "## GRL Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8264904c-7b24-45d4-a28d-83c9a3e3f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_coeff(iter_num, high=1.0, low=0.0, alpha=10.0, max_iter=10000.0):\n",
    "    return np.float32(2.0 * (high - low) / (1.0 + np.exp(-alpha*iter_num / max_iter)) - (high - low) + low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f964431d-fc82-4289-9bed-5dc4579d56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e442afbf-a606-4668-9c51-f4f708ab330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grl_hook(coeff):\n",
    "    def fun1(grad):\n",
    "        return -coeff*grad.clone()\n",
    "    return fun1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97abe545-337c-425e-88f0-8d93145dff62",
   "metadata": {},
   "source": [
    "# network part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3687700-0b2b-4792-844d-4949e77ca084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "  def __init__(self,passt_extractor,use_bottleneck=True, bottleneck_dim=256, new_cls=False, class_num=1000):\n",
    "    super(generator,self).__init__()\n",
    "    self.feature_extractor=passt_extractor\n",
    "    self.use_bottleneck = use_bottleneck\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.new_cls = new_cls\n",
    "      \n",
    "    if new_cls:\n",
    "        if self.use_bottleneck:\n",
    "            self.bottleneck = nn.Linear(768, bottleneck_dim)\n",
    "            self.fc = nn.Linear(bottleneck_dim, class_num)#g2\n",
    "            self.gvbg = nn.Linear(bottleneck_dim, class_num)#g3\n",
    "            self.focal1 = nn.Linear( class_num,class_num)#g3 weighting , like regularization , helps to learn how to raw class logits intract with eachother\n",
    "            self.focal2 = nn.Linear( class_num,1)#g3 weitghting\n",
    "            self.bottleneck.apply(init_weights)#dont know whaat kaiming is \n",
    "            self.fc.apply(init_weights)\n",
    "            self.gvbg.apply(init_weights)\n",
    "            self.__in_features = bottleneck_dim\n",
    "        else:\n",
    "            self.fc = nn.Linear(768, class_num)\n",
    "            self.fc.apply(init_weights)\n",
    "            self.gvbg = nn.Linear(768, class_num)\n",
    "            self.gvbg.apply(init_weights)\n",
    "            self.__in_features = 768\n",
    "    else:\n",
    "        self.fc = nn.Identity()\n",
    "        self.__in_features = 768\n",
    "\n",
    "  def forward(self, x, gvbg=True):\n",
    "    # print(\"input to \")  \n",
    "    x = self.feature_extractor(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "      \n",
    "    if self.use_bottleneck and self.new_cls:\n",
    "        x = self.bottleneck(x)\n",
    "    bridge = self.gvbg(x)\n",
    "    y = self.fc(x)\n",
    "      \n",
    "    if gvbg:\n",
    "        y = y - bridge\n",
    "        \n",
    "    return x, y, bridge\n",
    "\n",
    "  def output_num(self):\n",
    "    return self.__in_features\n",
    "\n",
    "  def get_parameters(self):\n",
    "    if self.new_cls:\n",
    "        if self.use_bottleneck:\n",
    "            parameter_list = [{\"params\":self.feature_extractor.parameters(), \"lr_mult\":1, 'decay_mult':2}, \\\n",
    "                            {\"params\":self.bottleneck.parameters(), \"lr_mult\":10, 'decay_mult':2}, \\\n",
    "                            {\"params\":self.fc.parameters(), \"lr_mult\":10, 'decay_mult':2}]\n",
    "        else:\n",
    "            parameter_list = [{\"params\":self.feature_extractor.parameters(), \"lr_mult\":1, 'decay_mult':2}, \\\n",
    "                            {\"params\":self.fc.parameters(), \"lr_mult\":10, 'decay_mult':2},\n",
    "                            {\"params\":self.gvbg.parameters(), \"lr_mult\":10, 'decay_mult':2}]\n",
    "    else:\n",
    "        parameter_list = [{\"params\":self.parameters(), \"lr_mult\":1, 'decay_mult':2}]\n",
    "    return parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "161ce4c9-2f31-455b-9f82-88822edaa615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: torch.Size([1, 256])\n",
      "y: torch.Size([1, 10])\n",
      "bridge: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# --- Run test ---\n",
    "\n",
    "model = generator(extractor,class_num=10,bottleneck_dim=256,new_cls=True)\n",
    "model.to(device)\n",
    "audio_tensor.to(device)\n",
    "features, y, bridge = model(audio_tensor)\n",
    "print(\"Feature:\", features.shape)\n",
    "print(\"y:\", y.shape)\n",
    "print(\"bridge:\", bridge.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8442ab2-5d0e-4418-9e95-05966be73ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialNetwork(nn.Module):\n",
    "  def __init__(self, in_feature, hidden_size):\n",
    "    super(AdversarialNetwork, self).__init__()\n",
    "    self.ad_layer1 = nn.Linear(in_feature, hidden_size)\n",
    "    self.ad_layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.ad_layer3 = nn.Linear(hidden_size, 1)\n",
    "    self.gvbd = nn.Linear(hidden_size, 1)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.relu2 = nn.ReLU()\n",
    "    self.dropout1 = nn.Dropout(0.5)\n",
    "    self.dropout2 = nn.Dropout(0.5)\n",
    "    self.dropout3 = nn.Dropout(0.5)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.apply(init_weights)\n",
    "    self.iter_num = 0\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.training:\n",
    "        self.iter_num += 1\n",
    "    coeff = calc_coeff(self.iter_num)\n",
    "    x = x * 1.0\n",
    "    x.register_hook(grl_hook(coeff))\n",
    "    x = self.ad_layer1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.ad_layer2(x)\n",
    "    x = self.relu2(x)\n",
    "    x = self.dropout2(x)\n",
    "    y = self.ad_layer3(x)\n",
    "    z = self.gvbd(x)\n",
    "    return y,z\n",
    "\n",
    "  def output_num(self):\n",
    "    return 1\n",
    "  def get_parameters(self):\n",
    "    return [{\"params\":self.parameters(), \"lr_mult\":10, 'decay_mult':2}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8c9a76-e96d-4e15-84ce-34fcbce3674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain logits tensor([[0.9948]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      "size torch.Size([1, 1])\n",
      "gvb logits tensor([[-2.9424]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      "size torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "discriminator = AdversarialNetwork(256,1024).to(device)\n",
    "discriminator.train()\n",
    "d1_output, d2_output = discriminator(features)\n",
    "print(\"domain logits\",d1_output,\"\\nsize\",d1_output.shape)\n",
    "print(\"gvb logits\",d2_output,\"\\nsize\",d2_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12054d-0a04-43d9-ad6a-d3e1d8ddad9d",
   "metadata": {},
   "source": [
    "<h2>learning rate scheduler<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11b39eda-1467-4ac6-8d6e-adbc2732bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inv_lr_scheduler(optimizer, iter_num, gamma, power, lr=0.001, weight_decay=0.0005):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = lr * (1 + gamma * iter_num) ** (-power)\n",
    "    i=0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr * param_group['lr_mult']\n",
    "        param_group['weight_decay'] = weight_decay * param_group['decay_mult']\n",
    "        i+=1\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "schedule_dict = {\"inv\":inv_lr_scheduler}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dceed4-4b9f-474f-b1f2-752e5ef2a48b",
   "metadata": {},
   "source": [
    "<h2>loss<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8850a249-1d3f-4d5c-b165-db8759317027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont know much about it \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "# domain discriminator loss \n",
    "class Myloss(nn.Module):\n",
    "    def __init__(self,epsilon=1e-8):\n",
    "        super(Myloss,self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        return\n",
    "    def forward(self,input_, label, weight):\n",
    "        entropy = - label * torch.log(input_ + self.epsilon) -(1 - label) * torch.log(1 - input_ + self.epsilon)\n",
    "        return torch.sum(entropy * weight)/2 \n",
    "    \n",
    "def Entropy(input_):\n",
    "    bs = input_.size(0)\n",
    "    epsilon = 1e-5\n",
    "    entropy = -input_ *torch.log(input_ + epsilon)\n",
    "    entropy = torch.sum(entropy, dim=1)\n",
    "    return entropy \n",
    "    \n",
    "def grl_hook(coeff):\n",
    "    def fun1(grad):\n",
    "        return -coeff*grad.clone()\n",
    "    return fun1\n",
    "    \n",
    "def GVB(input_list, ad_net, coeff=None, myloss=Myloss(),GVBD=False):\n",
    "    softmax_output = input_list[0]\n",
    "    focals = input_list[1].reshape(-1)\n",
    "    ad_out,fc_out = ad_net(softmax_output)\n",
    "    if GVBD==1:\n",
    "        ad_out = nn.Sigmoid()(ad_out - fc_out)\n",
    "    else:\n",
    "        ad_out = nn.Sigmoid()(ad_out)\n",
    "    batch_size = softmax_output.size(0) // 2\n",
    "    dc_target = torch.from_numpy(np.array([[1]] * batch_size + [[0]] * batch_size)).float().cuda()\n",
    "\n",
    "    x = softmax_output\n",
    "    entropy = Entropy(x)\n",
    "    entropy.register_hook(grl_hook(coeff))\n",
    "    entropy = torch.exp(-entropy)\n",
    "    mean_entropy = torch.mean(entropy)\n",
    "    gvbg = torch.mean(torch.abs(focals))\n",
    "    gvbd = torch.mean(torch.abs(fc_out))\n",
    "\n",
    "    source_mask = torch.ones_like(entropy)\n",
    "    source_mask[softmax_output.size(0)//2:] = 0\n",
    "    source_weight = entropy*source_mask\n",
    "    target_mask = torch.ones_like(entropy)\n",
    "    target_mask[0:softmax_output.size(0)//2] = 0\n",
    "    target_weight = entropy*target_mask\n",
    "    weight = source_weight / torch.sum(source_weight).detach().item() + \\\n",
    "             target_weight / torch.sum(target_weight).detach().item()\n",
    "    return myloss(ad_out,dc_target,weight.view(-1, 1)), mean_entropy, gvbg, gvbd \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090aebcd-75c2-4713-ab2b-9d9e48424082",
   "metadata": {},
   "source": [
    "<h2>data_list<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79765cf2-4cd1-4a5a-ad10-7b154877039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def make_dataset(audio_list, labels):#was image_list \n",
    "    if labels:\n",
    "      len_ = len(audio_list)\n",
    "      waves = [(audio_list[i].strip(), labels[i, :]) for i in range(len_)]  ### sure about this ? \n",
    "    else:\n",
    "      if len(audio_list[0].split()) > 2:\n",
    "        waves = [(val.split()[0], np.array([int(la) for la in val.split()[1:]])) for val in audio_list]\n",
    "      else:\n",
    "        waves = [(val.split()[0], int(val.split()[1])) for val in audio_list]\n",
    "    return waves # waves was images \n",
    "\n",
    "\n",
    "def audio_loader(path, sr=16000):\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    # Resample if needed\n",
    "    if sample_rate != sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform\n",
    "\n",
    "class AudioList(Dataset):\n",
    "    def __init__(self, audio_list, labels=None, transform=None, target_transform=None,sample_rate=16000):\n",
    "        self.wavs = make_dataset(audio_list, labels) #wavs was imgs\n",
    "        if len(self.wavs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 audio .wav files in the provides lists.\"))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.sample_rate=sample_rate\n",
    "\n",
    "\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.wavs[index]\n",
    "        wav = audio_loader(path,sr=self.sample_rate)\n",
    "        if self.transform is not None:\n",
    "            wav = self.transform(wav)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return wav, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc353a80-6444-4c59-9599-d8b654640327",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_source_list_labels.txt') as f:\n",
    "    audio_list= f.readlines()\n",
    "dataset=AudioList(audio_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75a754a8-7508-4ead-9ec6-c2ea8e059478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 160000]) tensor([6, 5, 5, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader =DataLoader(dataset,shuffle=True,batch_size=4)\n",
    "for w , l in loader :\n",
    "    print(w.shape,l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af382fd-2765-4d15-a789-3630028b23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_label_map = {\n",
    "    \"airport\": 0,\n",
    "    \"bus\": 1,\n",
    "    \"metro\": 2,\n",
    "    \"metro_station\": 3,\n",
    "    \"park\": 4,\n",
    "    \"public_square\": 5,\n",
    "    \"shopping_mall\": 6,\n",
    "    \"street_pedestrian\": 7,\n",
    "    \"street_traffic\": 8,\n",
    "    \"tram\": 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7ae73-ad00-4cb9-8cfa-1093fb087253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129c1b6e-e0a3-4365-9c85-3a617a338848",
   "metadata": {},
   "source": [
    "<h2>train<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40d91928-4003-42cb-9119-d5ef0a9e3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pdb\n",
    "import math\n",
    "import ipdb \n",
    "\n",
    "from packaging.version import Version\n",
    "\n",
    "\n",
    "## look later \n",
    "def audio_classification_test(loader, model, gvbg=False,key=\"test_source\"):\n",
    "    start_test = True\n",
    "    with torch.no_grad():\n",
    "       iter_test = iter(loader[key])\n",
    "       for i in range(len(loader[key])):\n",
    "           data = next(iter_test)\n",
    "           \n",
    "           inputs = data[0]\n",
    "           labels = data[1]\n",
    "           inputs = inputs.cuda()\n",
    "           labels = labels.cuda()\n",
    "           \n",
    "           #doing resizeing so it can be fead into passt to extract features\n",
    "           inputs_batch_size = inputs.shape[0]\n",
    "           inputs_data = inputs.shape[-1]\n",
    "           inputs = inputs.reshape(inputs_batch_size , inputs_data)\n",
    "           \n",
    "           _, outputs ,_  = model(inputs,gvbg=gvbg) \n",
    "           if start_test:\n",
    "               all_output = outputs.float()\n",
    "               all_label = labels.float()\n",
    "               start_test = False\n",
    "           else:\n",
    "               all_output = torch.cat((all_output, outputs.float()), 0)\n",
    "               all_label = torch.cat((all_label, labels.float()), 0)\n",
    "    _, predict = torch.max(all_output, 1)\n",
    "    accuracy = torch.sum(torch.squeeze(predict).float() == all_label).item() / float(all_label.size()[0]) ## squeeze\n",
    "    return accuracy\n",
    "\n",
    "def correct_path(file_list):\n",
    "    rm_string = '/DATA/disk1/hassassin/dataset/domain/OfficeHomeDataset/'\n",
    "    for i in range(len(file_list)): \n",
    "        file_list[i] = file_list[i].replace(rm_string , \"../data/office-home/\")\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "005c5bec-7336-4158-9ff4-d067e988a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    \n",
    "    ## set pre-process\n",
    "    #prep_dict = {\n",
    "    dsets = {}\n",
    "    dset_loaders = {}\n",
    "    data_config = config[\"data\"]\n",
    "    #prep_config = config[\"prep\"]\n",
    "    # prep_dict[\"source_train\"] = prep.image_target(**config[\"prep\"]['params'])\n",
    "    # prep_dict[\"target_train\"] = prep.image_target(**config[\"prep\"]['params'])\n",
    "    # prep_dict[\"source_test\"] = prep.image_test(**config[\"prep\"]['params'])\n",
    "    # prep_dict[\"target_test\"] = prep.image_test(**config[\"prep\"]['params'])\n",
    "\n",
    "    \n",
    "    ## prepare data\n",
    "    train_bs = data_config[\"source\"][\"batch_size\"]\n",
    "    test_bs = data_config[\"test\"][\"batch_size\"]\n",
    "    \n",
    "    ################## this is explictly used becasue i am lazy to remove the path in the text file####    \n",
    "    source_list = open(data_config[\"source\"][\"list_path\"]).readlines()\n",
    "    source_list = correct_path(source_list)\n",
    "    \n",
    "    tgt_list = open(data_config[\"target\"][\"list_path\"]).readlines()\n",
    "    tgt_list = correct_path(tgt_list)\n",
    "    \n",
    "    test_s_list = open(data_config[\"test_source\"][\"list_path\"]).readlines()\n",
    "    test_s_list = correct_path(test_s_list)\n",
    "    test_t_list = open(data_config[\"test_target\"][\"list_path\"]).readlines()\n",
    "    test_t_list = correct_path(test_t_list)\n",
    "    \n",
    "    \n",
    "    ##################################################################################################\n",
    "    dsets[\"source\"] = AudioList(source_list)\n",
    "    dset_loaders[\"source\"] = DataLoader(dsets[\"source\"], batch_size=train_bs, shuffle=True, num_workers=4, drop_last=True)\n",
    "    \n",
    "    dsets[\"target\"] = AudioList(tgt_list)\n",
    "    dset_loaders[\"target\"] = DataLoader(dsets[\"target\"], batch_size=train_bs, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    dsets[\"test_source\"] = AudioList(test_s_list)\n",
    "    dset_loaders[\"test_source\"] = DataLoader(dsets[\"test_source\"], batch_size=test_bs, shuffle=False, num_workers=4)\n",
    "    \n",
    "    dsets[\"test_target\"] = AudioList(test_t_list)\n",
    "    dset_loaders[\"test_target\"] = DataLoader(dsets[\"test_target\"], batch_size=test_bs, shuffle=False, num_workers=4)\n",
    "\n",
    "    ## set base network\n",
    "    class_num = config[\"network\"][\"params\"][\"class_num\"]\n",
    "    net_config = config[\"network\"]\n",
    "    base_network = net_config[\"name\"](**net_config[\"params\"])\n",
    "    base_network = base_network.cuda()\n",
    "\n",
    "    ## add additional network for some methods\n",
    "    ad_net = AdversarialNetwork( 10, 1024)\n",
    "    ad_net = ad_net.cuda()\n",
    " \n",
    "    ## set optimizer\n",
    "    parameter_list = base_network.get_parameters() + ad_net.get_parameters()\n",
    "    optimizer_config = config[\"optimizer\"]\n",
    "    optimizer = optimizer_config[\"type\"](parameter_list, \\\n",
    "                    **(optimizer_config[\"optim_params\"]))\n",
    "    param_lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_lr.append(param_group[\"lr\"])\n",
    "    schedule_param = optimizer_config[\"lr_param\"]\n",
    "    lr_scheduler = schedule_dict[optimizer_config[\"lr_type\"]] #######################\n",
    "\n",
    "    #multi gpu\n",
    "    gpus = config['gpu'].split(',') \n",
    "    if len(gpus) > 1:\n",
    "        ad_net = nn.DataParallel(ad_net, device_ids=[int(i) for i,k in enumerate(gpus)]) # dont know what this does \n",
    "        base_network = nn.DataParallel(base_network, device_ids=[int(i) for i,k in enumerate(gpus)])\n",
    "    \n",
    "    ## train   \n",
    "    len_train_source = len(dset_loaders[\"source\"])\n",
    "    len_train_target = len(dset_loaders[\"target\"])\n",
    "    transfer_loss_value = classifier_loss_value = total_loss_value = 0.0\n",
    "    best_acc = 0.0\n",
    "    for i in range(config[\"num_iterations\"]):\n",
    "        #test\n",
    "        if i % config[\"test_interval\"] == config[\"test_interval\"] - 1:\n",
    "            base_network.train(False)\n",
    "            \n",
    "            acc_s = audio_classification_test(dset_loaders, base_network, gvbg=config[\"GVBG\"], key=\"test_source\")#was temp_acc\n",
    "            acc_t = audio_classification_test(dset_loaders, base_network, gvbg=config[\"GVBG\"], key=\"test_target\")\n",
    "            \n",
    "            temp_model = nn.Sequential(base_network)\n",
    "            if acc_t > best_acc:\n",
    "                best_acc = acc_t\n",
    "                best_model = temp_model\n",
    "            log_str = \"iter: {:05d}, source_acc: {:.5f}, target_acc: {:.5f}\".format(i, acc_s, acc_t)#was log_str = \"iter: {:05d}, precision: {:.5f}\".format(i, temp_acc)\n",
    "            config[\"out_file\"].write(log_str+\"\\n\")\n",
    "            config[\"out_file\"].flush()\n",
    "            print(log_str)\n",
    "        #save model\n",
    "        if i % config[\"snapshot_interval\"] == 0:\n",
    "            torch.save(base_network.state_dict(), osp.join(config[\"output_path\"], \\\n",
    "                \"iter_{:05d}_model.pth.tar\".format(i)))\n",
    "\n",
    "        ## train one iter\n",
    "        base_network.train(True)\n",
    "        ad_net.train(True)\n",
    "        loss_params = config[\"loss\"]                  \n",
    "        optimizer = lr_scheduler(optimizer, i, **schedule_param)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #dataloader\n",
    "        if i % len_train_source == 0:\n",
    "            iter_source = iter(dset_loaders[\"source\"])\n",
    "        if i % len_train_target == 0:\n",
    "            iter_target = iter(dset_loaders[\"target\"])\n",
    "            \n",
    "        #network\n",
    "        inputs_source, labels_source = next(iter_source)\n",
    "        inputs_target, _ = next(iter_target)\n",
    "        # print(\"input source\",inputs_source.shape)\n",
    "        # print(\"input target\",inputs_target.shape)\n",
    "        \n",
    "        inputs_source, inputs_target, labels_source = inputs_source.cuda(), inputs_target.cuda(), labels_source.cuda()\n",
    "        # print(\"after cuda shape\",inputs_source.shape)\n",
    "\n",
    "        \n",
    "        source_batch_size_dataloader = inputs_source.shape[0]\n",
    "        tgt_batch_size_dataloader = inputs_target.shape[0]\n",
    "        audio_size_dataloader = inputs_source.shape[-1]\n",
    "        inputs_source = inputs_source.reshape(source_batch_size_dataloader , audio_size_dataloader)\n",
    "        inputs_target = inputs_target.reshape(tgt_batch_size_dataloader , audio_size_dataloader) \n",
    "        features_source, outputs_source, focal_source = base_network(inputs_source,gvbg=config[\"GVBG\"])\n",
    "        features_target, outputs_target, focal_target = base_network(inputs_target,gvbg=config[\"GVBG\"])\n",
    "        features = torch.cat((features_source, features_target), dim=0)\n",
    "        outputs = torch.cat((outputs_source, outputs_target), dim=0)\n",
    "        focals = torch.cat((focal_source,focal_target),dim=0)\n",
    "        softmax_out = nn.Softmax(dim=1)(outputs)\n",
    "\n",
    "        #loss calculation\n",
    "        \n",
    "        transfer_loss, mean_entropy, gvbg, gvbd = GVB([softmax_out,focals], ad_net, calc_coeff(i), GVBD=config['GVBD'])\n",
    "        classifier_loss = nn.CrossEntropyLoss()(outputs_source, labels_source)\n",
    "        total_loss = loss_params[\"trade_off\"] * transfer_loss + classifier_loss + config[\"GVBG\"] * gvbg + abs(config['GVBD']) * gvbd\n",
    "\n",
    "        if i % config[\"print_num\"] == 0:\n",
    "            log_str = \"iter: {:05d}, transferloss: {:.5f}, classifier_loss: {:.5f}, mean entropy:{:.5f}, gvbg:{:.5f}, gvbd:{:.5f}\".format(i, transfer_loss, classifier_loss, mean_entropy, gvbg, gvbd)\n",
    "            config[\"out_file\"].write(log_str+\"\\n\")\n",
    "            config[\"out_file\"].flush()\n",
    "            print(log_str)\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(best_model, osp.join(config[\"output_path\"], \"best_model.pth.tar\"))\n",
    "    return best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f2bac-1218-4c8a-973d-ee609acc359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "iter: 00000, transferloss: 0.67508, classifier_loss: 0.39530, mean entropy:0.51333, gvbg:0.49823, gvbd:0.82561\n",
      "iter: 00100, transferloss: 1.30132, classifier_loss: 0.52701, mean entropy:0.60603, gvbg:0.19104, gvbd:0.60151\n",
      "iter: 00200, transferloss: 0.53631, classifier_loss: 0.12656, mean entropy:0.72923, gvbg:0.09774, gvbd:0.32336\n",
      "iter: 00300, transferloss: 0.61623, classifier_loss: 0.40660, mean entropy:0.52012, gvbg:0.09854, gvbd:0.09855\n",
      "iter: 00400, transferloss: 0.72418, classifier_loss: 0.06869, mean entropy:0.76811, gvbg:0.08434, gvbd:0.13143\n",
      "iter: 00499, source_acc: 0.76061, target_acc: 0.47005\n",
      "iter: 00500, transferloss: 0.69916, classifier_loss: 0.74263, mean entropy:0.61767, gvbg:0.07199, gvbd:0.06846\n",
      "iter: 00600, transferloss: 0.70352, classifier_loss: 0.94860, mean entropy:0.69468, gvbg:0.08576, gvbd:0.03904\n",
      "iter: 00700, transferloss: 0.72695, classifier_loss: 0.06642, mean entropy:0.69821, gvbg:0.08429, gvbd:0.04155\n",
      "iter: 00800, transferloss: 0.52392, classifier_loss: 0.12362, mean entropy:0.69694, gvbg:0.07371, gvbd:0.04316\n",
      "iter: 00900, transferloss: 0.74225, classifier_loss: 0.21547, mean entropy:0.52131, gvbg:0.07598, gvbd:0.04774\n",
      "iter: 00999, source_acc: 0.70909, target_acc: 0.43480\n",
      "iter: 01000, transferloss: 0.64958, classifier_loss: 0.88186, mean entropy:0.75236, gvbg:0.07164, gvbd:0.02153\n",
      "iter: 01100, transferloss: 0.70791, classifier_loss: 0.45587, mean entropy:0.71506, gvbg:0.07497, gvbd:0.01976\n",
      "iter: 01200, transferloss: 0.55043, classifier_loss: 0.01422, mean entropy:0.89452, gvbg:0.08518, gvbd:0.03370\n",
      "iter: 01300, transferloss: 0.66506, classifier_loss: 0.61175, mean entropy:0.90652, gvbg:0.07185, gvbd:0.02724\n",
      "iter: 01400, transferloss: 0.63646, classifier_loss: 0.17681, mean entropy:0.78265, gvbg:0.06216, gvbd:0.02111\n",
      "iter: 01499, source_acc: 0.77576, target_acc: 0.38287\n",
      "iter: 01500, transferloss: 0.68868, classifier_loss: 0.99211, mean entropy:0.76087, gvbg:0.05716, gvbd:0.01863\n",
      "iter: 01600, transferloss: 0.74750, classifier_loss: 0.76022, mean entropy:0.72049, gvbg:0.07217, gvbd:0.01745\n",
      "iter: 01700, transferloss: 0.68718, classifier_loss: 0.12294, mean entropy:0.82455, gvbg:0.05209, gvbd:0.01747\n",
      "iter: 01800, transferloss: 0.46961, classifier_loss: 0.06101, mean entropy:0.89847, gvbg:0.06081, gvbd:0.03080\n",
      "iter: 01900, transferloss: 0.65623, classifier_loss: 0.18316, mean entropy:0.67623, gvbg:0.04778, gvbd:0.01244\n",
      "iter: 01999, source_acc: 0.76364, target_acc: 0.44503\n",
      "iter: 02000, transferloss: 0.65767, classifier_loss: 0.21468, mean entropy:0.91010, gvbg:0.08655, gvbd:0.00779\n",
      "iter: 02100, transferloss: 0.65360, classifier_loss: 0.06438, mean entropy:0.77067, gvbg:0.05593, gvbd:0.00682\n",
      "iter: 02200, transferloss: 0.71226, classifier_loss: 0.64487, mean entropy:0.91264, gvbg:0.04947, gvbd:0.01081\n",
      "iter: 02300, transferloss: 0.62166, classifier_loss: 0.71358, mean entropy:0.81407, gvbg:0.04077, gvbd:0.01099\n",
      "iter: 02400, transferloss: 0.67673, classifier_loss: 0.32994, mean entropy:0.80823, gvbg:0.04772, gvbd:0.00231\n",
      "iter: 02499, source_acc: 0.76061, target_acc: 0.46247\n",
      "iter: 02500, transferloss: 0.61404, classifier_loss: 0.93429, mean entropy:0.77005, gvbg:0.06214, gvbd:0.01177\n",
      "iter: 02600, transferloss: 0.70647, classifier_loss: 0.06383, mean entropy:0.91681, gvbg:0.03507, gvbd:0.00948\n",
      "iter: 02700, transferloss: 0.60573, classifier_loss: 0.67663, mean entropy:0.81242, gvbg:0.05010, gvbd:0.01093\n",
      "iter: 02800, transferloss: 0.61003, classifier_loss: 2.00502, mean entropy:0.88948, gvbg:0.05328, gvbd:0.01209\n",
      "iter: 02900, transferloss: 0.59421, classifier_loss: 0.45236, mean entropy:0.85138, gvbg:0.03530, gvbd:0.01884\n",
      "iter: 02999, source_acc: 0.73939, target_acc: 0.40637\n",
      "iter: 03000, transferloss: 0.64536, classifier_loss: 0.03157, mean entropy:0.87714, gvbg:0.07202, gvbd:0.01236\n",
      "iter: 03100, transferloss: 0.60984, classifier_loss: 0.07831, mean entropy:0.79625, gvbg:0.06632, gvbd:0.01073\n",
      "iter: 03200, transferloss: 0.66918, classifier_loss: 0.25857, mean entropy:0.88938, gvbg:0.03854, gvbd:0.00550\n",
      "iter: 03300, transferloss: 0.74281, classifier_loss: 0.26864, mean entropy:0.79347, gvbg:0.03639, gvbd:0.01187\n",
      "iter: 03400, transferloss: 0.67191, classifier_loss: 0.39867, mean entropy:0.79419, gvbg:0.04934, gvbd:0.00486\n",
      "iter: 03499, source_acc: 0.79394, target_acc: 0.40409\n",
      "iter: 03500, transferloss: 0.73386, classifier_loss: 0.88046, mean entropy:0.76055, gvbg:0.05292, gvbd:0.00916\n",
      "iter: 03600, transferloss: 0.74524, classifier_loss: 0.34718, mean entropy:0.86038, gvbg:0.03582, gvbd:0.00296\n",
      "iter: 03700, transferloss: 0.75811, classifier_loss: 0.03205, mean entropy:0.92572, gvbg:0.04225, gvbd:0.00375\n",
      "iter: 03800, transferloss: 0.71013, classifier_loss: 0.02030, mean entropy:0.86956, gvbg:0.04557, gvbd:0.00929\n",
      "iter: 03900, transferloss: 0.71584, classifier_loss: 0.00031, mean entropy:0.99292, gvbg:0.06952, gvbd:0.00539\n",
      "iter: 03999, source_acc: 0.76364, target_acc: 0.43366\n",
      "iter: 04000, transferloss: 0.68353, classifier_loss: 0.08111, mean entropy:0.75193, gvbg:0.04460, gvbd:0.00416\n",
      "iter: 04100, transferloss: 0.63317, classifier_loss: 0.00996, mean entropy:0.96418, gvbg:0.03474, gvbd:0.00716\n",
      "iter: 04200, transferloss: 0.73482, classifier_loss: 0.01404, mean entropy:0.89395, gvbg:0.05917, gvbd:0.02336\n",
      "iter: 04300, transferloss: 0.65890, classifier_loss: 0.00047, mean entropy:0.94332, gvbg:0.03342, gvbd:0.00236\n",
      "iter: 04400, transferloss: 0.66540, classifier_loss: 0.26426, mean entropy:0.92327, gvbg:0.03820, gvbd:0.00500\n",
      "iter: 04499, source_acc: 0.77273, target_acc: 0.49356\n",
      "iter: 04500, transferloss: 0.72547, classifier_loss: 0.05153, mean entropy:0.93849, gvbg:0.03475, gvbd:0.00759\n",
      "iter: 04600, transferloss: 0.67718, classifier_loss: 0.00065, mean entropy:0.93815, gvbg:0.03739, gvbd:0.00508\n",
      "iter: 04700, transferloss: 0.64287, classifier_loss: 0.07581, mean entropy:0.78347, gvbg:0.03124, gvbd:0.00179\n",
      "iter: 04800, transferloss: 0.64457, classifier_loss: 0.58295, mean entropy:0.76383, gvbg:0.04156, gvbd:0.00468\n",
      "iter: 04900, transferloss: 0.66976, classifier_loss: 0.70247, mean entropy:0.81824, gvbg:0.02639, gvbd:0.00317\n",
      "iter: 04999, source_acc: 0.77576, target_acc: 0.46437\n",
      "iter: 05000, transferloss: 0.72148, classifier_loss: 0.19579, mean entropy:0.87581, gvbg:0.04407, gvbd:0.00504\n",
      "iter: 05100, transferloss: 0.70323, classifier_loss: 0.00107, mean entropy:0.92454, gvbg:0.02815, gvbd:0.00395\n",
      "iter: 05200, transferloss: 0.69925, classifier_loss: 0.36100, mean entropy:0.87113, gvbg:0.03175, gvbd:0.00264\n",
      "iter: 05300, transferloss: 0.67803, classifier_loss: 0.50568, mean entropy:0.88332, gvbg:0.02932, gvbd:0.00365\n",
      "iter: 05400, transferloss: 0.69348, classifier_loss: 0.09778, mean entropy:0.90473, gvbg:0.04198, gvbd:0.00565\n",
      "iter: 05499, source_acc: 0.76061, target_acc: 0.45148\n",
      "iter: 05500, transferloss: 0.67156, classifier_loss: 0.07623, mean entropy:0.73038, gvbg:0.03758, gvbd:0.00491\n",
      "iter: 05600, transferloss: 0.65006, classifier_loss: 1.57751, mean entropy:0.71619, gvbg:0.04089, gvbd:0.00538\n",
      "iter: 05700, transferloss: 0.62317, classifier_loss: 0.00849, mean entropy:0.97913, gvbg:0.03712, gvbd:0.00823\n",
      "iter: 05800, transferloss: 0.72930, classifier_loss: 0.68679, mean entropy:0.72850, gvbg:0.03267, gvbd:0.00312\n",
      "iter: 05900, transferloss: 0.70054, classifier_loss: 0.05774, mean entropy:0.91082, gvbg:0.02875, gvbd:0.00115\n",
      "iter: 05999, source_acc: 0.75758, target_acc: 0.18916\n",
      "iter: 06000, transferloss: 0.65263, classifier_loss: 0.03847, mean entropy:0.93511, gvbg:0.02704, gvbd:0.00169\n",
      "iter: 06100, transferloss: 0.71398, classifier_loss: 0.07202, mean entropy:0.86587, gvbg:0.02902, gvbd:0.00348\n",
      "iter: 06200, transferloss: 0.52743, classifier_loss: 0.01833, mean entropy:0.95369, gvbg:0.02524, gvbd:0.00352\n",
      "iter: 06300, transferloss: 0.73542, classifier_loss: 0.00156, mean entropy:0.97259, gvbg:0.02772, gvbd:0.00408\n",
      "iter: 06400, transferloss: 0.74878, classifier_loss: 0.01242, mean entropy:0.97559, gvbg:0.02493, gvbd:0.00468\n",
      "iter: 06499, source_acc: 0.76970, target_acc: 0.36012\n",
      "iter: 06500, transferloss: 0.68394, classifier_loss: 1.09205, mean entropy:0.96244, gvbg:0.02916, gvbd:0.00484\n",
      "iter: 06600, transferloss: 0.75135, classifier_loss: 0.00124, mean entropy:0.87298, gvbg:0.03022, gvbd:0.00872\n",
      "iter: 06700, transferloss: 0.75440, classifier_loss: 0.13222, mean entropy:0.87814, gvbg:0.03415, gvbd:0.00321\n",
      "iter: 06800, transferloss: 0.75576, classifier_loss: 0.00049, mean entropy:0.97483, gvbg:0.05249, gvbd:0.00624\n",
      "iter: 06900, transferloss: 0.65928, classifier_loss: 0.03071, mean entropy:0.95500, gvbg:0.02959, gvbd:0.00211\n",
      "iter: 06999, source_acc: 0.77273, target_acc: 0.32942\n",
      "iter: 07000, transferloss: 0.71122, classifier_loss: 0.01884, mean entropy:0.95379, gvbg:0.02335, gvbd:0.00306\n",
      "iter: 07100, transferloss: 0.65382, classifier_loss: 0.62837, mean entropy:0.96538, gvbg:0.02341, gvbd:0.00354\n",
      "iter: 07200, transferloss: 0.62220, classifier_loss: 0.03834, mean entropy:0.82636, gvbg:0.01584, gvbd:0.00160\n",
      "iter: 07300, transferloss: 0.70468, classifier_loss: 0.00014, mean entropy:0.88378, gvbg:0.02781, gvbd:0.00505\n"
     ]
    }
   ],
   "source": [
    "__name__ = True\n",
    "if __name__ == True:\n",
    "\n",
    "    assert Version(torch.__version__) >= Version('1.0.0'), 'PyTorch>=1.0.0 is required'\n",
    "    ###### Update the parameters ###########\n",
    "    class Args:\n",
    "        gpu_id ='0'\n",
    "        net = 'passt'\n",
    "        dset = 'dcase'\n",
    "        s_train_path = 'train_source_list_labels.txt'\n",
    "        t_train_path = 'train_target_list_labels.txt'\n",
    "        s_test_path = 'test_source_list_labels.txt'\n",
    "        t_test_path = 'test_target_list_labels.txt'\n",
    "        test_interval = 500\n",
    "        snapshot_interval = 5000\n",
    "        print_num = 100\n",
    "        num_iterations = 30002\n",
    "        output_dir = 'output'\n",
    "        lr = 0.001\n",
    "        trade_off = 1\n",
    "        batch_size = 4\n",
    "        GVBG = 1\n",
    "        GVBD = 1\n",
    "        CDAN = False         \n",
    "    args = Args()\n",
    "  \n",
    "    \n",
    "     \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\n",
    "\n",
    "    # train config\n",
    "    config = {}\n",
    "    config[\"GVBG\"] = args.GVBG\n",
    "    config[\"GVBD\"] = args.GVBD \n",
    "    config[\"CDAN\"] = args.CDAN\n",
    "    config[\"gpu\"] = args.gpu_id\n",
    "    config[\"num_iterations\"] = args.num_iterations \n",
    "    config[\"print_num\"] = args.print_num\n",
    "    config[\"test_interval\"] = args.test_interval\n",
    "    config[\"snapshot_interval\"] = args.snapshot_interval\n",
    "    config[\"output_for_test\"] = True\n",
    "    config[\"output_path\"] = args.dset + \"/\" + args.output_dir\n",
    "\n",
    "    if not osp.exists(config[\"output_path\"]):\n",
    "        os.system('mkdir -p '+config[\"output_path\"])\n",
    "    config[\"out_file\"] = open(osp.join(config[\"output_path\"], \"log.txt\"), \"w\")\n",
    "    if not osp.exists(config[\"output_path\"]):\n",
    "        os.mkdir(config[\"output_path\"])\n",
    "\n",
    "\n",
    "    #config[\"prep\"] = {'params':{\"resize_size\":256, \"crop_size\":224, 'alexnet':False}}\n",
    "    config[\"loss\"] = {\"trade_off\":args.trade_off}\n",
    "    config[\"network\"] = {\n",
    "    \"name\": generator,\n",
    "    \"params\": {\n",
    "        \"passt_extractor\": extractor,\n",
    "        \"use_bottleneck\": True,\n",
    "        \"bottleneck_dim\": 256,\n",
    "        \"new_cls\": True,\n",
    "        \"class_num\": 10\n",
    "            }\n",
    "        }\n",
    "\n",
    "    config[\"optimizer\"] = {\"type\":optim.SGD, \"optim_params\":{'lr':args.lr, \"momentum\":0.9, \\\n",
    "                           \"weight_decay\":0.0005, \"nesterov\":True}, \"lr_type\":\"inv\", \\\n",
    "                           \"lr_param\":{\"lr\":args.lr, \"gamma\":0.001, \"power\":0.75} }\n",
    "\n",
    "    config[\"dataset\"] = args.dset\n",
    "    config[\"data\"] = {\n",
    "        \"source\": {\"list_path\": args.s_train_path, \"batch_size\": args.batch_size},\n",
    "        \"target\": {\"list_path\": args.t_train_path, \"batch_size\": args.batch_size},\n",
    "        \"test_source\": {\"list_path\": args.s_test_path, \"batch_size\": args.batch_size},\n",
    "        \"test_target\": {\"list_path\": args.t_test_path, \"batch_size\": args.batch_size},\n",
    "        \"test\": {\"batch_size\": args.batch_size}\n",
    "                     }\n",
    "    seed=2025\n",
    "\n",
    "    # if config[\"dataset\"] == \"office-home\":\n",
    "    #     seed = 2019\n",
    "    #     config[\"optimizer\"][\"lr_param\"][\"lr\"] = 0.001 # optimal parameters\n",
    "    #     config[\"network\"][\"params\"][\"class_num\"] = 65\n",
    "    # elif config[\"dataset\"] == \"office\":\n",
    "    #     seed = 2019\n",
    "    #     if   (\"webcam\" in args.s_dset_path and \"amazon\" in args.t_dset_path) or \\\n",
    "    #          (\"dslr\" in args.s_dset_path and \"amazon\" in args.t_dset_path):\n",
    "    #          config[\"optimizer\"][\"lr_param\"][\"lr\"] = 0.001 # optimal parameters\n",
    "    #     elif (\"amazon\" in args.s_dset_path and \"webcam\" in args.t_dset_path) or \\\n",
    "    #          (\"amazon\" in args.s_dset_path and \"dslr\" in args.t_dset_path) or \\\n",
    "    #          (\"webcam\" in args.s_dset_path and \"dslr\" in args.t_dset_path) or \\\n",
    "    #          (\"dslr\" in args.s_dset_path and \"webcam\" in args.t_dset_path):\n",
    "    #          config[\"optimizer\"][\"lr_param\"][\"lr\"] = 0.0003 # optimal parameters\n",
    "    #     config[\"network\"][\"params\"][\"class_num\"] = 31\n",
    "    # elif config[\"dataset\"] == \"visda\":\n",
    "    #     seed = 9297\n",
    "    #     config[\"optimizer\"][\"lr_param\"][\"lr\"] = 0.0003 # optimal parameters\n",
    "    #     config[\"network\"][\"params\"][\"class_num\"] = 12\n",
    "    # else:\n",
    "    #     raise ValueError('Dataset cannot be recognized. Please define your own dataset here.')\n",
    "    print(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    config[\"out_file\"].write(str(config))\n",
    "    config[\"out_file\"].flush()\n",
    "    train(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190222a-f028-4448-9aab-659b37d7ea16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222fd7ec-23ae-472c-92d8-ee8c43c46cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioList(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.data = [(line.split()[0], int(line.split()[1])) for line in file_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, label = self.data[index]\n",
    "        waveform, sr = torchaudio.load(path)  \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        return waveform, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d27902-10e4-47fd-acd0-ae5f38733766",
   "metadata": {},
   "source": [
    "<h2>text file creation script<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee1f8b-af73-4a20-b8ab-4931cc931965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2888bb7-0d8e-4e02-9cbf-0c770a525406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wav_filenames(input_dir, output_txt_path, full_path=True):\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.wav'):\n",
    "                    path = os.path.join(root, file) if full_path else file\n",
    "                    f.write(path + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_directory = './G3/data/corrected_split/test/target'           # Change this to your .wav folder\n",
    "output_file = 'test_target_list.txt'         # Desired output text file\n",
    "save_wav_filenames(input_directory, output_file, full_path=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a624532-ff1b-469f-89bd-63f55af7ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_target_list_labels.txt') as f:\n",
    "    audio_list= f.readlines()\n",
    "audio_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72cb08-6d50-4f88-a328-402889d25184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcase_label(path):\n",
    "    for scene, label in scene_label_map.items():\n",
    "        if scene in path:\n",
    "            return label\n",
    "    return -1 \n",
    "\n",
    "with open(\"test_target_list.txt\") as fin, open(\"test_target_list_labels.txt\", \"w\") as fout:\n",
    "    for line in fin:\n",
    "        path = line.strip()\n",
    "        label = get_dcase_label(path)\n",
    "        if label == -1:\n",
    "            print(f\"Warning: Scene not found in {path}\")\n",
    "            continue\n",
    "        fout.write(f\"{path} {label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456dec23-941d-478e-9eb9-33efcccf6ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb6163-370f-4c55-b6a4-3ea72d824aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7b45a-0fee-42ff-bdc8-5284a11a526c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c038a3f-c2ec-4d03-ba7b-115222065759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
